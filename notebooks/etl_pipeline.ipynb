{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef0844b8",
   "metadata": {},
   "source": [
    "# Veterinary Business Intelligence System - ETL Pipeline (Task 2)\n",
    "\n",
    "This notebook implements the Data Requirement, System Design & Development phase of the project.\n",
    "It covers:\n",
    "1.  **Data Extraction**: Fetching data from FDA API, Dog API, and a supplementary CSV source.\n",
    "2.  **Staging**: Loading raw data into a staging environment (Pandas DataFrames).\n",
    "3.  **Transformation**: Cleaning, normalizing, and enriching the data.\n",
    "4.  **Data Warehousing**: Designing and populating a Star Schema in a local SQLite database.\n",
    "5.  **Validation**: Running queries to ensure data quality and business relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "676b9cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported and directories setup.\n"
     ]
    }
   ],
   "source": [
    "# 1. Setup and Library Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import sqlite3\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Configuration\n",
    "DATA_DIR = '../data'\n",
    "RAW_DIR = os.path.join(DATA_DIR, 'raw')\n",
    "PROCESSED_DIR = os.path.join(DATA_DIR, 'processed')\n",
    "DB_PATH = os.path.join(DATA_DIR, 'veterinary_dw.db')\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(RAW_DIR, exist_ok=True)\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Libraries imported and directories setup.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fffedac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data from FDA API from period 20110107 to 20260103\n",
      "This may take a while for full data.\n",
      "Fetched 1000 records so far. Total fetched: 1000\n",
      "Fetched 1000 records so far. Total fetched: 2000\n",
      "Fetched 1000 records so far. Total fetched: 3000\n",
      "Fetched 1000 records so far. Total fetched: 4000\n",
      "Fetched 1000 records so far. Total fetched: 5000\n",
      "Fetched 1000 records so far. Total fetched: 6000\n",
      "Fetched 1000 records so far. Total fetched: 7000\n",
      "Fetched 1000 records so far. Total fetched: 8000\n",
      "Fetched 1000 records so far. Total fetched: 9000\n",
      "Fetched 1000 records so far. Total fetched: 10000\n",
      "Reached the limit of 10000 records.\n",
      "../data/raw/fda_adverse_events.json\n",
      "Raw FDA data saved to ../data/raw/fda_adverse_events.json\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# 2. Data Extraction: FDA Adverse Event API\n",
    "\n",
    "def fetch_fda_data(max_records_to_fetch, limit=500, api_key=None):\n",
    "    \"\"\"\n",
    "    Fetches adverse event data from the openFDA Animal & Veterinary API.\n",
    "    Note: For a full historical load (15 years), we would need to implement pagination\n",
    "    and loop through 'skip' parameters. For this demo, we fetch a sample.\n",
    "    \"\"\"\n",
    "    base_url = \"https://api.fda.gov/animalandveterinary/event.json\"\n",
    "\n",
    "    # Query for the last 15 years (approximate start date)\n",
    "    start_date = (datetime.now() - timedelta(days=15*365)).strftime('%Y%m%d')\n",
    "    end_date = datetime.now().strftime('%Y%m%d')\n",
    "\n",
    "    params = {\n",
    "        'search': f'original_receive_date:[{start_date} TO {end_date}]',\n",
    "        'limit': limit,\n",
    "        'sort': 'original_receive_date:desc'\n",
    "    }\n",
    "\n",
    "    if api_key:\n",
    "        params['api_key'] = api_key\n",
    "\n",
    "    all_results = []\n",
    "    total_fetched = 0\n",
    "\n",
    "    session = requests.Session()\n",
    "    current_url = base_url\n",
    "\n",
    "    use_params_flag = True # this is used only on the first request, after that we use the next link as it contains params\n",
    "\n",
    "    print(f\"Fetching data from FDA API from period {start_date} to {end_date}\\nThis may take a while for full data.\")\n",
    "    while True:\n",
    "        try:\n",
    "            # If it's the first request, use params.\n",
    "            # If it's a subsequent request (via Link header), params are already in the URL.\n",
    "            if use_params_flag:\n",
    "                response = session.get(current_url, params=params)\n",
    "            else:\n",
    "                response = session.get(current_url)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "\n",
    "            batch = data.get('results', [])\n",
    "            if not batch:\n",
    "                break\n",
    "\n",
    "            all_results.extend(batch)\n",
    "            total_fetched += len(batch)\n",
    "            print(f\"Fetched {len(batch)} records so far. Total fetched: {total_fetched}\")\n",
    "\n",
    "            # Check for the next page as the FDA gives a link header with the url for next page of results\n",
    "            if \"next\" in response.links:\n",
    "                current_url = response.links[\"next\"][\"url\"]\n",
    "                # Here sleep for a bit (I don't want to DDOS the FDA... That would be rude)\n",
    "                use_params_flag = False\n",
    "                time.sleep(1)\n",
    "\n",
    "            else:\n",
    "                print(\"No more pages to fetch.\")\n",
    "                break\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"HTTP error occurred: {e}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            break\n",
    "        if total_fetched >= max_records_to_fetch:\n",
    "            print(f\"Reached the limit of {max_records_to_fetch} records.\")\n",
    "            break\n",
    "\n",
    "    return all_results\n",
    "# Fetch sample data\n",
    "# Optional: Add your API key here if you have one, e.g., api_key=\"YOUR_KEY\"\n",
    "fda_raw_data = fetch_fda_data(max_records_to_fetch=10000, limit=1000, api_key=None)\n",
    "\n",
    "# Save raw data to JSON for staging\n",
    "fda_raw_path = os.path.join(RAW_DIR, 'fda_adverse_events.json')\n",
    "print(fda_raw_path)\n",
    "with open(fda_raw_path, 'w') as f:\n",
    "    json.dump(fda_raw_data, f)\n",
    "print(f\"Raw FDA data saved to {fda_raw_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c40290fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully fetched 170 dog breeds.\n",
      "Raw Dog breed data saved to ../data/raw/dog_breeds.json\n"
     ]
    }
   ],
   "source": [
    "# 3. Data Extraction: Dog Breed API\n",
    "\n",
    "def fetch_dog_breeds(api_key=None):\n",
    "    \"\"\"\n",
    "    Fetches dog breed classifications from TheDogAPI.\n",
    "    \"\"\"\n",
    "    url = \"https://api.thedogapi.com/v1/breeds\"\n",
    "    headers = {'x-api-key': api_key} if api_key else {}\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        breeds = response.json()\n",
    "        print(f\"Successfully fetched {len(breeds)} dog breeds.\")\n",
    "        return breeds\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching dog breeds: {e}\")\n",
    "        return []\n",
    "\n",
    "# Optional: Add your API key here if you have one\n",
    "dog_breeds_raw = fetch_dog_breeds(api_key=None)\n",
    "\n",
    "# Save raw data\n",
    "dog_raw_path = os.path.join(RAW_DIR, 'dog_breeds.json')\n",
    "with open(dog_raw_path, 'w') as f:\n",
    "    json.dump(dog_breeds_raw, f)\n",
    "print(f\"Raw Dog breed data saved to {dog_raw_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e880b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supplementary data file not found!\n"
     ]
    }
   ],
   "source": [
    "# 4. Data Extraction: Supplementary Breed Data (CSV)\n",
    "\n",
    "# We are using a local CSV file for Cat Breeds as the supplementary source.\n",
    "# This file was created in the 'data/raw' directory.\n",
    "\n",
    "cat_breeds_path = os.path.join(RAW_DIR, 'cat_breeds.csv')\n",
    "\n",
    "if os.path.exists(cat_breeds_path):\n",
    "    print(f\"Supplementary data found at {cat_breeds_path}\")\n",
    "else:\n",
    "    print(\"Supplementary data file not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d95bfe4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FDA Staging Data Shape: (10000, 19)\n",
      "Dog Staging Data Shape: (170, 13)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/raw/cat_breeds.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDog Staging Data Shape:\u001b[39m\u001b[33m\"\u001b[39m, df_dog_staging.shape)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Load Cat Data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m df_cat_staging = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcat_breeds_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCat Staging Data Shape:\u001b[39m\u001b[33m\"\u001b[39m, df_cat_staging.shape)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Basic Schema Validation\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/BussinesInt/lib/python3.14/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/BussinesInt/lib/python3.14/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/BussinesInt/lib/python3.14/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/BussinesInt/lib/python3.14/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/BussinesInt/lib/python3.14/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../data/raw/cat_breeds.csv'"
     ]
    }
   ],
   "source": [
    "# 5. Staging Area: Raw Data Inspection and Schema Validation\n",
    "\n",
    "# Load FDA Data\n",
    "df_fda_staging = pd.DataFrame(fda_raw_data)\n",
    "print(\"FDA Staging Data Shape:\", df_fda_staging.shape)\n",
    "\n",
    "# Load Dog Data\n",
    "df_dog_staging = pd.DataFrame(dog_breeds_raw)\n",
    "print(\"Dog Staging Data Shape:\", df_dog_staging.shape)\n",
    "\n",
    "# Load Cat Data\n",
    "df_cat_staging = pd.read_csv(cat_breeds_path)\n",
    "print(\"Cat Staging Data Shape:\", df_cat_staging.shape)\n",
    "\n",
    "# Basic Schema Validation\n",
    "required_fda_columns = ['original_receive_date', 'animal', 'drug', 'reaction']\n",
    "missing_cols = [col for col in required_fda_columns if col not in df_fda_staging.columns]\n",
    "if missing_cols:\n",
    "    print(f\"Warning: Missing columns in FDA data: {missing_cols}\")\n",
    "else:\n",
    "    print(\"FDA Schema Validation Passed.\")\n",
    "\n",
    "df_fda_staging.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7910201c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FDA Data Cleaned.\n",
      "  event_date   gender receiver_country receiver_state\n",
      "0 2011-10-05     Male              USA             MD\n",
      "1 2011-04-20   Female              USA             MD\n",
      "2 2011-03-17     Male              USA             MD\n",
      "3 2011-07-21  Unknown              USA             MD\n",
      "4 2015-06-29   Female              USA             MD\n"
     ]
    }
   ],
   "source": [
    "# 6. Transformation: Cleaning FDA Event Data\n",
    "\n",
    "def clean_fda_data(df):\n",
    "    # 1. Flatten 'animal' column\n",
    "    # The 'animal' column contains a dictionary (gender, breed, weight, etc.)\n",
    "    if 'animal' in df.columns:\n",
    "        animal_df = pd.json_normalize(df['animal'])\n",
    "        # Rename columns to avoid collision if needed, or just merge\n",
    "        df = pd.concat([df.drop(['animal'], axis=1), animal_df], axis=1)\n",
    "\n",
    "    # 2. Flatten 'receiver' column for Geography\n",
    "    if 'receiver' in df.columns:\n",
    "        receiver_df = pd.json_normalize(df['receiver'])\n",
    "        # Keep only relevant location columns\n",
    "        loc_cols = ['city', 'state', 'country', 'postal_code']\n",
    "        for col in loc_cols:\n",
    "            if col in receiver_df.columns:\n",
    "                df[f'receiver_{col}'] = receiver_df[col]\n",
    "            else:\n",
    "                df[f'receiver_{col}'] = 'Unknown'\n",
    "        df.drop(['receiver'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "    # 3. Handle Missing Values (MSK)\n",
    "    # Replace 'MSK' with NaN or a placeholder\n",
    "    df.replace('MSK', np.nan, inplace=True)\n",
    "\n",
    "    # 4. Standardize Dates\n",
    "    df['event_date'] = pd.to_datetime(df['original_receive_date'], format='%Y%m%d', errors='coerce')\n",
    "\n",
    "    # 5. Standardize Gender\n",
    "    if 'gender' in df.columns:\n",
    "        df['gender'] = df['gender'].fillna('Unknown').str.title()\n",
    "\n",
    "    # 6. Standardize Reproductive Status\n",
    "    if 'reproductive_status' in df.columns:\n",
    "        df['reproductive_status'] = df['reproductive_status'].fillna('Unknown').str.title()\n",
    "\n",
    "    return df\n",
    "\n",
    "df_fda_cleaned = clean_fda_data(df_fda_staging.copy())\n",
    "print(\"FDA Data Cleaned.\")\n",
    "# Check for new geography columns\n",
    "print(df_fda_cleaned[['event_date', 'gender', 'receiver_country', 'receiver_state']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b23ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drugs Normalized. New Shape: (786, 64)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand_name</th>\n",
       "      <th>active_ingredient_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MSK</td>\n",
       "      <td>Milbemycin, Lufenuron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MSK</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MSK</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MSK</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MSK</td>\n",
       "      <td>Carprofen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  brand_name active_ingredient_normalized\n",
       "0        MSK        Milbemycin, Lufenuron\n",
       "1        MSK                             \n",
       "2        MSK                             \n",
       "3        MSK                             \n",
       "4        MSK                    Carprofen"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. Transformation: Normalizing Active Ingredients\n",
    "\n",
    "def normalize_drugs(df):\n",
    "    \"\"\"\n",
    "    Explodes the 'drug' list and extracts active ingredients.\n",
    "    \"\"\"\n",
    "    # Explode the list of drugs so each drug gets a row\n",
    "    df_exploded = df.explode('drug')\n",
    "\n",
    "    # Normalize the drug dictionary\n",
    "    drug_details = pd.json_normalize(df_exploded['drug'])\n",
    "\n",
    "    # We need to keep the original index to merge back or just concat if indices align\n",
    "    # Reset index of exploded df to align with drug_details\n",
    "    df_exploded = df_exploded.reset_index(drop=True)\n",
    "    drug_details = drug_details.reset_index(drop=True)\n",
    "\n",
    "    # Combine\n",
    "    df_combined = pd.concat([df_exploded.drop(['drug'], axis=1), drug_details], axis=1)\n",
    "\n",
    "    # Extract Active Ingredients\n",
    "    # The API structure for active ingredients can be complex.\n",
    "    # Often it's a list of dicts inside the drug dict.\n",
    "    # For simplicity, we will try to extract 'active_ingredients' list and join them string\n",
    "\n",
    "    def extract_ingredients(ingredients_list):\n",
    "        if isinstance(ingredients_list, list):\n",
    "            names = [item.get('name', '') for item in ingredients_list if 'name' in item]\n",
    "            return \", \".join(names)\n",
    "        return \"Unknown\"\n",
    "\n",
    "    if 'active_ingredients' in df_combined.columns:\n",
    "        df_combined['active_ingredient_normalized'] = df_combined['active_ingredients'].apply(extract_ingredients)\n",
    "    else:\n",
    "        df_combined['active_ingredient_normalized'] = \"Unknown\"\n",
    "\n",
    "    return df_combined\n",
    "\n",
    "df_fda_drugs = normalize_drugs(df_fda_cleaned)\n",
    "print(\"Drugs Normalized. New Shape:\", df_fda_drugs.shape)\n",
    "df_fda_drugs[['brand_name', 'active_ingredient_normalized']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90c1c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breed columns found: ['breed.is_crossbred', 'breed.breed_component']\n",
      "Data Enriched.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>breed.breed_component</th>\n",
       "      <th>breed_group</th>\n",
       "      <th>bred_for</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Retriever - Labrador</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Retriever - Labrador</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Retriever - Labrador</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Retriever - Labrador</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Retriever - Labrador</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  breed.breed_component breed_group bred_for\n",
       "0  Retriever - Labrador     Unknown  Unknown\n",
       "1  Retriever - Labrador     Unknown  Unknown\n",
       "2  Retriever - Labrador     Unknown  Unknown\n",
       "3  Retriever - Labrador     Unknown  Unknown\n",
       "4  Retriever - Labrador     Unknown  Unknown"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8. Transformation: Enriching with Breed Groups and Purpose\n",
    "\n",
    "# Prepare Dog Breed Data for Merge\n",
    "# We want to map 'breed.breed_component' from FDA to 'name' in Dog API to get 'breed_group' and 'bred_for'\n",
    "df_dog_lookup = df_dog_staging[['name', 'breed_group', 'bred_for']].copy()\n",
    "df_dog_lookup['name'] = df_dog_lookup['name'].str.lower()\n",
    "\n",
    "# Prepare FDA Data\n",
    "# The breed information is in 'breed.breed_component' (which might be a string or list)\n",
    "# After flattening in step 6, it might be 'breed.breed_component'\n",
    "# Let's inspect columns to find breed info\n",
    "breed_col = [c for c in df_fda_drugs.columns if 'breed' in c.lower()]\n",
    "print(\"Breed columns found:\", breed_col)\n",
    "\n",
    "# Assuming 'breed.breed_component' is the column and it might be a string\n",
    "if 'breed.breed_component' in df_fda_drugs.columns:\n",
    "    df_fda_drugs['breed_lower'] = df_fda_drugs['breed.breed_component'].astype(str).str.lower()\n",
    "\n",
    "    # Merge\n",
    "    df_enriched = pd.merge(\n",
    "        df_fda_drugs,\n",
    "        df_dog_lookup,\n",
    "        left_on='breed_lower',\n",
    "        right_on='name',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Fill NaN for non-dogs or unmatched breeds\n",
    "    df_enriched['breed_group'] = df_enriched['breed_group'].fillna('Unknown')\n",
    "    df_enriched['bred_for'] = df_enriched['bred_for'].fillna('Unknown')\n",
    "else:\n",
    "    print(\"Breed column not found as expected. Skipping enrichment.\")\n",
    "    df_enriched = df_fda_drugs\n",
    "    df_enriched['breed_group'] = 'Unknown'\n",
    "    df_enriched['bred_for'] = 'Unknown'\n",
    "\n",
    "print(\"Data Enriched.\")\n",
    "df_enriched[['breed.breed_component', 'breed_group', 'bred_for']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ac6e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advanced Data Warehouse Schema Created with Indexes and New Dimensions.\n"
     ]
    }
   ],
   "source": [
    "# 9. Data Warehouse Schema Design (Advanced Star Schema)\n",
    "\n",
    "# Connect to SQLite Database\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Drop tables if they exist\n",
    "cursor.executescript(\"\"\"\n",
    "DROP TABLE IF EXISTS FactAdverseEvents;\n",
    "DROP TABLE IF EXISTS DimTime;\n",
    "DROP TABLE IF EXISTS DimAnimal;\n",
    "DROP TABLE IF EXISTS DimDrug;\n",
    "DROP TABLE IF EXISTS DimReaction;\n",
    "DROP TABLE IF EXISTS DimGeography;\n",
    "DROP TABLE IF EXISTS DimOutcome;\n",
    "\"\"\")\n",
    "\n",
    "# Create Advanced Dimension Tables\n",
    "create_dims_sql = \"\"\"\n",
    "-- Enhanced Time Dimension\n",
    "CREATE TABLE DimTime (\n",
    "    TimeKey INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    Date DATE UNIQUE,\n",
    "    Year INTEGER,\n",
    "    Month INTEGER,\n",
    "    Day INTEGER,\n",
    "    Quarter INTEGER,\n",
    "    DayOfWeek TEXT,\n",
    "    IsWeekend BOOLEAN\n",
    ");\n",
    "\n",
    "-- Geography Dimension\n",
    "CREATE TABLE DimGeography (\n",
    "    GeographyKey INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    City TEXT,\n",
    "    State TEXT,\n",
    "    Country TEXT,\n",
    "    PostalCode TEXT\n",
    ");\n",
    "\n",
    "-- Outcome Dimension\n",
    "CREATE TABLE DimOutcome (\n",
    "    OutcomeKey INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    OutcomeName TEXT UNIQUE,\n",
    "    SeverityLevel TEXT\n",
    ");\n",
    "\n",
    "-- Animal Dimension\n",
    "CREATE TABLE DimAnimal (\n",
    "    AnimalKey INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    Species TEXT,\n",
    "    Breed TEXT,\n",
    "    Gender TEXT,\n",
    "    ReproductiveStatus TEXT,\n",
    "    BreedGroup TEXT,\n",
    "    BreedPurpose TEXT\n",
    ");\n",
    "\n",
    "-- Drug Dimension\n",
    "CREATE TABLE DimDrug (\n",
    "    DrugKey INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    BrandName TEXT,\n",
    "    ActiveIngredients TEXT,\n",
    "    Manufacturer TEXT\n",
    ");\n",
    "\n",
    "-- Reaction Dimension\n",
    "CREATE TABLE DimReaction (\n",
    "    ReactionKey INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    ReactionName TEXT UNIQUE\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Create Fact Table with Foreign Keys and Indexes\n",
    "# Added DaysToOnset to cover \"How many days it takes for the reactions to appear\"\n",
    "create_fact_sql = \"\"\"\n",
    "CREATE TABLE FactAdverseEvents (\n",
    "    EventID INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    FDA_ReportID TEXT,\n",
    "    TimeKey INTEGER,\n",
    "    AnimalKey INTEGER,\n",
    "    DrugKey INTEGER,\n",
    "    ReactionKey INTEGER,\n",
    "    GeographyKey INTEGER,\n",
    "    OutcomeKey INTEGER,\n",
    "    Age FLOAT,\n",
    "    Weight FLOAT,\n",
    "    DaysToOnset INTEGER, -- New field for reaction latency analysis\n",
    "    -- Measures\n",
    "    ReactionCount INTEGER DEFAULT 1,\n",
    "    FOREIGN KEY(TimeKey) REFERENCES DimTime(TimeKey),\n",
    "    FOREIGN KEY(AnimalKey) REFERENCES DimAnimal(AnimalKey),\n",
    "    FOREIGN KEY(DrugKey) REFERENCES DimDrug(DrugKey),\n",
    "    FOREIGN KEY(ReactionKey) REFERENCES DimReaction(ReactionKey),\n",
    "    FOREIGN KEY(GeographyKey) REFERENCES DimGeography(GeographyKey),\n",
    "    FOREIGN KEY(OutcomeKey) REFERENCES DimOutcome(OutcomeKey)\n",
    ");\n",
    "\n",
    "-- Create Indexes for Performance\n",
    "CREATE INDEX idx_fact_time ON FactAdverseEvents(TimeKey);\n",
    "CREATE INDEX idx_fact_animal ON FactAdverseEvents(AnimalKey);\n",
    "CREATE INDEX idx_fact_drug ON FactAdverseEvents(DrugKey);\n",
    "CREATE INDEX idx_fact_geo ON FactAdverseEvents(GeographyKey);\n",
    "\"\"\"\n",
    "\n",
    "cursor.executescript(create_dims_sql)\n",
    "cursor.executescript(create_fact_sql)\n",
    "conn.commit()\n",
    "print(\"Advanced Data Warehouse Schema Created with Indexes and New Dimensions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4705dbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIX: Populate the Time Dimension before loading facts\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def populate_time_dimension(start_year=2000, end_year=2025):\n",
    "    print(\"Populating DimTime...\")\n",
    "\n",
    "    # Connect (ensure we use the same connection object name as your script)\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    start_date = datetime(start_year, 1, 1)\n",
    "    end_date = datetime(end_year, 12, 31)\n",
    "    current_date = start_date\n",
    "\n",
    "    batch_data = []\n",
    "    while current_date <= end_date:\n",
    "        is_weekend = current_date.weekday() >= 5 # 5=Sat, 6=Sun\n",
    "        batch_data.append((\n",
    "            str(current_date.date()), # Date string\n",
    "            current_date.year,\n",
    "            current_date.month,\n",
    "            current_date.day,\n",
    "            (current_date.month - 1) // 3 + 1, # Quarter\n",
    "            current_date.strftime('%A'), # DayOfWeek\n",
    "            is_weekend\n",
    "        ))\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "    # Bulk Insert\n",
    "    cursor.executemany(\"\"\"\n",
    "        INSERT OR IGNORE INTO DimTime\n",
    "        (Date, Year, Month, Day, Quarter, DayOfWeek, IsWeekend)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\", batch_data)\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(\"âœ… DimTime populated successfully.\")\n",
    "\n",
    "# Run the fix\n",
    "populate_time_dimension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5f9cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DimTime Populated.\n",
      "DimReaction Populated.\n",
      "DimOutcome will be populated dynamically.\n",
      "DimGeography will be populated dynamically.\n"
     ]
    }
   ],
   "source": [
    "# 10. Load: Populating Dimension Tables\n",
    "\n",
    "def get_or_create_key(table, search_cols, values, key_col):\n",
    "    where_clause = \" AND \".join([f\"{col} = ?\" for col in search_cols])\n",
    "    query = f\"SELECT {key_col} FROM {table} WHERE {where_clause}\"\n",
    "    cursor.execute(query, values)\n",
    "    result = cursor.fetchone()\n",
    "\n",
    "    if result:\n",
    "        return result[0]\n",
    "    else:\n",
    "        cols = \", \".join(search_cols)\n",
    "        placeholders = \", \".join([\"?\" for _ in search_cols])\n",
    "        insert_sql = f\"INSERT INTO {table} ({cols}) VALUES ({placeholders})\"\n",
    "        cursor.execute(insert_sql, values)\n",
    "        return cursor.lastrowid\n",
    "\n",
    "# 1. Populate DimTime (Enhanced)\n",
    "unique_dates = df_enriched['event_date'].dropna().unique()\n",
    "for date_val in unique_dates:\n",
    "    dt = pd.to_datetime(date_val)\n",
    "    day_name = dt.day_name()\n",
    "    is_weekend = 1 if dt.weekday() >= 5 else 0\n",
    "\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT OR IGNORE INTO DimTime (Date, Year, Month, Day, Quarter, DayOfWeek, IsWeekend)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\", (dt.date(), dt.year, dt.month, dt.day, dt.quarter, day_name, is_weekend))\n",
    "conn.commit()\n",
    "print(\"DimTime Populated.\")\n",
    "\n",
    "# 2. Populate DimReaction\n",
    "if 'reaction' in df_enriched.columns:\n",
    "    all_reactions = set()\n",
    "    for reactions in df_enriched['reaction']:\n",
    "        if isinstance(reactions, list):\n",
    "            for r in reactions:\n",
    "                if 'name' in r:\n",
    "                    all_reactions.add(r['name'])\n",
    "\n",
    "    for r_name in all_reactions:\n",
    "        cursor.execute(\"INSERT OR IGNORE INTO DimReaction (ReactionName) VALUES (?)\", (r_name,))\n",
    "    conn.commit()\n",
    "print(\"DimReaction Populated.\")\n",
    "\n",
    "# 3. Populate DimOutcome (New)\n",
    "# Extract unique outcomes from the 'outcome' column (which might be a list or dict in raw data)\n",
    "# In flattened data, it might be 'outcome.medical_status' or just 'outcome'\n",
    "# Let's inspect the column in the loop or assume it's 'outcome'\n",
    "# For now, we'll populate it dynamically in the Fact Load loop or pre-scan here.\n",
    "# Let's pre-scan if possible, or just use get_or_create_key in the loop.\n",
    "print(\"DimOutcome will be populated dynamically.\")\n",
    "\n",
    "# 4. Populate DimGeography (New)\n",
    "# Similarly, we can populate dynamically or pre-scan.\n",
    "print(\"DimGeography will be populated dynamically.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c996b740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Fact Table... this may take a moment.\n",
      "Finished loading Advanced Fact Table. Processed 786 events.\n"
     ]
    }
   ],
   "source": [
    "# 11. Load: Populating Fact Table (Advanced)\n",
    "\n",
    "print(\"Loading Fact Table... this may take a moment.\")\n",
    "\n",
    "count = 0\n",
    "for index, row in df_enriched.iterrows():\n",
    "    try:\n",
    "        # 1. Time Key\n",
    "        event_date = row['event_date']\n",
    "        if pd.isnull(event_date):\n",
    "            continue\n",
    "        dt = pd.to_datetime(event_date)\n",
    "        cursor.execute(\"SELECT TimeKey FROM DimTime WHERE Date = ?\", (str(dt.date()),))\n",
    "        time_res = cursor.fetchone()\n",
    "        time_key = time_res[0] if time_res else None\n",
    "\n",
    "        # 2. Animal Key\n",
    "        species = row.get('species', 'Unknown')\n",
    "        breed = row.get('breed.breed_component', 'Unknown')\n",
    "        gender = row.get('gender', 'Unknown')\n",
    "        repro_status = row.get('reproductive_status', 'Unknown')\n",
    "        breed_group = row.get('breed_group', 'Unknown')\n",
    "        breed_purpose = row.get('bred_for', 'Unknown')\n",
    "\n",
    "        animal_key = get_or_create_key(\n",
    "            'DimAnimal',\n",
    "            ['Species', 'Breed', 'Gender', 'ReproductiveStatus', 'BreedGroup', 'BreedPurpose'],\n",
    "            (str(species), str(breed), str(gender), str(repro_status), str(breed_group), str(breed_purpose)),\n",
    "            'AnimalKey'\n",
    "        )\n",
    "\n",
    "        # 3. Drug Key\n",
    "        brand = row.get('brand_name', 'Unknown')\n",
    "        active_ing = row.get('active_ingredient_normalized', 'Unknown')\n",
    "        manufacturer = row.get('manufacturer.name', 'Unknown')\n",
    "\n",
    "        drug_key = get_or_create_key(\n",
    "            'DimDrug',\n",
    "            ['BrandName', 'ActiveIngredients', 'Manufacturer'],\n",
    "            (str(brand), str(active_ing), str(manufacturer)),\n",
    "            'DrugKey'\n",
    "        )\n",
    "\n",
    "        # 4. Geography Key\n",
    "        city = row.get('receiver_city', 'Unknown')\n",
    "        state = row.get('receiver_state', 'Unknown')\n",
    "        country = row.get('receiver_country', 'Unknown')\n",
    "        postal = row.get('receiver_postal_code', 'Unknown')\n",
    "\n",
    "        geo_key = get_or_create_key(\n",
    "            'DimGeography',\n",
    "            ['City', 'State', 'Country', 'PostalCode'],\n",
    "            (str(city), str(state), str(country), str(postal)),\n",
    "            'GeographyKey'\n",
    "        )\n",
    "\n",
    "        # 5. Outcome Key\n",
    "        outcome_raw = row.get('outcome', [])\n",
    "        outcome_name = \"Unknown\"\n",
    "        if isinstance(outcome_raw, list) and len(outcome_raw) > 0:\n",
    "             outcome_name = outcome_raw[0].get('medical_status', 'Unknown')\n",
    "        elif isinstance(outcome_raw, str):\n",
    "            outcome_name = outcome_raw\n",
    "\n",
    "        severity = \"Normal\"\n",
    "        if outcome_name.lower() in ['died', 'euthanized']:\n",
    "            severity = \"Critical\"\n",
    "\n",
    "        outcome_key = get_or_create_key(\n",
    "            'DimOutcome',\n",
    "            ['OutcomeName', 'SeverityLevel'],\n",
    "            (str(outcome_name), severity),\n",
    "            'OutcomeKey'\n",
    "        )\n",
    "\n",
    "        # 6. Reaction Key & Fact Insertion\n",
    "        reactions_list = row.get('reaction', [])\n",
    "        if isinstance(reactions_list, list):\n",
    "            for r in reactions_list:\n",
    "                r_name = r.get('name', 'Unknown')\n",
    "\n",
    "                # Get Reaction Key\n",
    "                cursor.execute(\"SELECT ReactionKey FROM DimReaction WHERE ReactionName = ?\", (r_name,))\n",
    "                res = cursor.fetchone()\n",
    "                if not res:\n",
    "                    cursor.execute(\"INSERT INTO DimReaction (ReactionName) VALUES (?)\", (r_name,))\n",
    "                    reaction_key = cursor.lastrowid\n",
    "                else:\n",
    "                    reaction_key = res[0]\n",
    "\n",
    "                # Insert Fact\n",
    "                fda_id = row.get('report_id', 'Unknown')\n",
    "                age = row.get('age.min', 0)\n",
    "                weight = row.get('weight.min', 0)\n",
    "\n",
    "                # Calculate DaysToOnset (if onset_date exists)\n",
    "                # Note: This field might not exist in all API responses, so we handle gracefully\n",
    "                days_to_onset = None\n",
    "                if 'onset_date' in row and pd.notnull(row['onset_date']):\n",
    "                    try:\n",
    "                        onset_dt = pd.to_datetime(row['onset_date'], format='%Y%m%d', errors='coerce')\n",
    "                        if pd.notnull(onset_dt):\n",
    "                            days_to_onset = (onset_dt - dt).days\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                cursor.execute(\"\"\"\n",
    "                    INSERT INTO FactAdverseEvents\n",
    "                    (FDA_ReportID, TimeKey, AnimalKey, DrugKey, ReactionKey, GeographyKey, OutcomeKey, Age, Weight, DaysToOnset)\n",
    "                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "                \"\"\", (str(fda_id), time_key, animal_key, drug_key, reaction_key, geo_key, outcome_key, age, weight, days_to_onset))\n",
    "\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            conn.commit()\n",
    "\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "conn.commit()\n",
    "print(f\"Finished loading Advanced Fact Table. Processed {count} events.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfc750e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Analysis: Geographic Distribution (Top Countries) ---\n",
      "  Country  EventCount\n",
      "0     USA        2461\n",
      "\n",
      "--- Analysis: Critical Outcomes by Breed Group ---\n",
      "  BreedGroup  CriticalCount\n",
      "0    Unknown            719\n",
      "1        Toy             20\n",
      "2      Hound             16\n",
      "\n",
      "--- Analysis: Seasonality (Events by Month) ---\n",
      "    Month  Count\n",
      "0       1     78\n",
      "1       2     83\n",
      "2       3    233\n",
      "3       4    281\n",
      "4       5    293\n",
      "5       6    111\n",
      "6       7    308\n",
      "7       8    196\n",
      "8       9    234\n",
      "9      10    189\n",
      "10     11    340\n",
      "11     12    115\n"
     ]
    }
   ],
   "source": [
    "# 12. Data Quality Checks and Validation (Advanced Queries)\n",
    "\n",
    "# Query 1: Geographic Distribution of Events\n",
    "print(\"--- Analysis: Geographic Distribution (Top Countries) ---\")\n",
    "query_geo = \"\"\"\n",
    "SELECT\n",
    "    g.Country,\n",
    "    COUNT(f.EventID) as EventCount\n",
    "FROM FactAdverseEvents f\n",
    "JOIN DimGeography g ON f.GeographyKey = g.GeographyKey\n",
    "GROUP BY g.Country\n",
    "ORDER BY EventCount DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "print(pd.read_sql_query(query_geo, conn))\n",
    "\n",
    "# Query 2: Critical Outcomes by Breed Group\n",
    "print(\"\\n--- Analysis: Critical Outcomes by Breed Group ---\")\n",
    "query_outcome = \"\"\"\n",
    "SELECT\n",
    "    a.BreedGroup,\n",
    "    COUNT(f.EventID) as CriticalCount\n",
    "FROM FactAdverseEvents f\n",
    "JOIN DimAnimal a ON f.AnimalKey = a.AnimalKey\n",
    "JOIN DimOutcome o ON f.OutcomeKey = o.OutcomeKey\n",
    "WHERE o.SeverityLevel = 'Critical'\n",
    "GROUP BY a.BreedGroup\n",
    "ORDER BY CriticalCount DESC;\n",
    "\"\"\"\n",
    "print(pd.read_sql_query(query_outcome, conn))\n",
    "\n",
    "# Query 3: Seasonal Analysis (Events by Month)\n",
    "print(\"\\n--- Analysis: Seasonality (Events by Month) ---\")\n",
    "query_season = \"\"\"\n",
    "SELECT\n",
    "    t.Month,\n",
    "    COUNT(f.EventID) as Count\n",
    "FROM FactAdverseEvents f\n",
    "JOIN DimTime t ON f.TimeKey = t.TimeKey\n",
    "GROUP BY t.Month\n",
    "ORDER BY t.Month;\n",
    "\"\"\"\n",
    "print(pd.read_sql_query(query_season, conn))\n",
    "\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BussinesInt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
